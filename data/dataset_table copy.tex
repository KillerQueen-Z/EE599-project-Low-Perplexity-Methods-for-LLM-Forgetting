\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{float} % For [H] position control
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}



\setcounter{secnumdepth}{0} 

\title{Low-Perplexity Methods for Tackling LLM Forgetting and Improving Efficiency}
\author {
    % Authors
    Jianzhong Ren
    Zhe Chen
}
\affiliations {
    % Affiliations
    jianzhon@usc.edu, zchen116@usc.edu
}
\begin{document}

\maketitle


\begin{abstract}
Catastrophic forgetting commonly occurs during supervised fine-tuning of large language models, especially in continuous fine-tuning settings. This project introduces a optimized method based on continuous
fine-tuning (CFT) scenarios using low perplexity which controls how tokens contribute to parameter updates based on token-level confidence and perplexity. By applying masking strategies that regulate high- and low-perplexity tokens, the proposed method improves training stability and reduces forgetting without requiring synthetic data generation or architectural changes. Experiment results across multiple reasoning and QA datasets show improved knowledge retention and fine-tuning efficiency compared to standard methods.
\end{abstract}

\section{Introduction}

Large language models (LLMS) have demonstrated extraordinary capabilities in a wide range of tasks, from natural language understanding to reasoning and code generation. To adapt these general models to specific downstream fields, supervised fine-tuning (SFT) has become a standard and effective practice. However, fine-tuning often introduces a well-known challenge: catastrophic forgetting, where improvements on a target task come at the cost of degraded performance on previously mastered, non-target tasks\cite{wu2025mitigatingforgettingllmfinetuning}.\\The objective of this project is to reduce catastrophic forgetting during fine-tuning without relying on synthetic data generation or complex replay mechanisms. We did not modify the model architecture or introduce additional training phases, but focused on controlling how the training data affected parameter updates during standard supervised fine-tuning. Our overall plan is to make the fine-tuning process confusing. During the training process, we first measure the token-level confidence and perplexity from the output distribution of the model. Then aggregate these values onto short token sets to obtain more stable confidence estimates at different inference stages. Based on these measurements, we designed a masking strategy to selectively adjust the contribution of tokens with different degrees of confusion to the training loss. Specifically, we explored two masking strategies: suppressing the impact of high-confusion tokens that are more likely to cause unstable updates, and emphasizing low-confusion tokens that are more consistent with the pre-training distribution of the model. These strategies are implemented in a continuous fine-tuning framework using LoRA, without the need to change the model architecture or the behavior during inference. \\To evaluate the effectiveness of the proposed method, we conducted controlled fine-tuning experiments on multiple datasets covering mathematical reasoning, question answering, and common sense tasks. After consecutive fine-tuning phases, we evaluated the performance of the target task and the retention of the previous learning ability. Through these experiments, our goal is to prove that confusion-aware masking can enhance training stability, reduce forgetting, and improve fine-tuning efficiency.

\section{Background}

Recent studies have shown that fine-tuning with LLM-generated data, such as self-generated or rephrased responses, can significantly alleviate catastrophic forgetting while maintaining or even improving target-task performance\cite{wang-etal-2023-self-instruct}. Compared to conventional supervised fine-tuning on human-annotated ground truth, models trained on synthetic responses often exhibit better retention of general capabilities across non-target tasks\cite{cordeiro2023williamsconjectureholdsmeteor,mukherjee2023orcaprogressivelearningcomplex}. This phenomenon has been observed consistently across multiple domains and model families, suggesting that the advantage of LLM-generated data is not incidental but reflects a deeper property of the fine-tuning process. Despite these empirical successes, the mechanism underlying this robustness remains insufficiently explained. Existing explanations often attribute the benefits of synthetic data to higher semantic consistency, stylistic alignment with the base model, or implicit knowledge distillation effects\cite{Gou_2021}. However, such explanations are largely qualitative and do not provide a principled criterion for when and why LLM-generated data outperforms ground truth. Moreover, reliance on large external models for data generation introduces practical concerns, including substantial computational cost, limited scalability, potential label noise, and increased system complexity. A key insight emerging from recent work is that token-level perplexity plays a critical role in fine-tuning stability and forgetting behavior\cite{Whitty_2022}.Compared to human-written ground truth, LLM-generated responses consistently exhibit lower average perplexity and fewer high-perplexity token spikes. These high-perplexity tokens often correspond to rare phrasing patterns, unconventional reasoning steps, or stylistic variations that deviate from the model’s learned prior distribution\cite{holtzman2020curiouscaseneuraltext}. When used in supervised fine-tuning, such tokens induce larger gradients, higher training loss, and more aggressive parameter updates\cite{dodge2020finetuningpretrainedlanguagemodels}. Training on low-perplexity sequences, by contrast, leads to smoother optimization dynamics. Lower token-level uncertainty translates into reduced training loss, milder weight updates, and smaller deviations from the pretrained model parameters. As a result, the model can acquire task-specific knowledge while largely preserving representations that support general-purpose reasoning, thereby mitigating performance degradation on non-target tasks. Importantly, this perspective re-frames catastrophic forgetting not merely as a consequence of task interference, but as a byproduct of statistical mismatch between fine-tuning data and the model’s prior distribution. This analysis suggests that the advantage of LLM-generated data does not stem solely from improved semantic quality or stylistic similarity, but from its statistical compatibility with the base model, as quantified by token-level perplexity. Consequently, the core benefit of synthetic data may be achievable without explicit data generation, by directly controlling or filtering high-perplexity tokens during training. This insight opens the door to more efficient and principled fine-tuning strategies that reduce forgetting while avoiding the computational and practical limitations of replay- or generation-based approaches.

\section{Approach}
Our approach provides a fine-grained, model-intrinsic assessment of reasoning quality by jointly analyzing confidence dynamics and perplexity. Without relying on external supervision or ground-truth annotations, this framework enables systematic investigation of how uncertainty evolves throughout the reasoning process and how localized confidence degradation contributes to reasoning failure.
\subsection{Confidence}
Confidence measures how certain a model is about its own output and Reflects the model’s self-estimated reliability during generation or reasoning. High confidence indicates model is highly certain about the next token, producing a sharp probability distribution with the top token’s probability near 1. Conversely, low confidence is characterized by a relatively flat probability distribution, in which probability mass is spread across many candidate tokens without a clear dominant choice. Token confidence can be defined as the negative average log-probability of the top k tokens at position i:
\begin{equation}
C_i = -\frac{1}{k} \sum_{j=1}^{k} \log P_i(j)
\end{equation}
where k denotes the number of top tokens considered. High confidence corresponds to peaked distributions and greater model certainty, while low confidence indicates uncertainty in token prediction.\\
Instead of focusing on individual tokens or the entire response, the average confidence is computed within a sliding window\cite{fu2025deepthinkconfidence}. This provides a smoother and more accurate reflection of the model’s confidence in a specific reasoning stage. For each group $G_i$, group confidence is defined as:
\begin{equation}
C_{G_i} = \frac{1}{|G_i|} \sum_{t \in G_i} C_t
\end{equation}
where $|G_i|$ denotes the number of tokens in group $G_i$.\\
Aimed at characterizing the influence of extremely low-confidence groups, the bottom 10\% group confidence is defined as the average of the bottom 10\% groups $G_b$:
\begin{equation}
C_{bottom-10}(t)
=
\frac{1}{|G_b|}
\sum_{G_j \in G_b} C_{G_j}
\end{equation}
\\
The lowest group confidence represents the confidence of the least confident group within a reasoning trace, which signifies points where the model’s reasoning “stalls” or fails.
\begin{equation}
C_{least}(t) = \min_{G_j \in G} C_{G_j}
\end{equation}
\\
Reasoning quality often degrades toward the end of long chains of thought. Therefore, tail confidence focuses on the confidence level of the last part of the reasoning process. It is defined as follows:
\begin{equation}
C_{t{tail}}(t)
=
\frac{1}{|T_{{tail}}|}
\sum_{t \in T_{{tail}}} C_t
\end{equation}
where $T_{tail}$ represents a fixed number of tokens.\\
\subsection{Perplexity}
Perplexity is a widely used metric for quantifying the uncertainty of probabilistic sequence models, particularly in language modeling. It can be interpreted as the effective number of equally likely choices the model considers at each prediction step. Perplexity($PPL$) is defined as the exponentiated average negative log-likelihood of a model over a sequence:
\begin{equation}
\mathrm{PPL}
=
\exp\!\left(
-\frac{1}{N}
\sum_{i=1}^{N}
\log P\!\left(w_i \mid w_1, \ldots, w_{i-1}\right)
\right),
\label{eq:perplexity}
\end{equation}
\\where $P(w_i \mid w_1, \ldots, w_{i-1})$ denotes the conditional probability of token $w_i$
given its preceding context, and $N$ is the total number of tokens in the sequence. Lower perplexity indicates that the model assigns higher probability to the observed tokens and thus exhibits more confident and reliable predictions.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LaTeX/pipline.png}
    \caption{Train pipline}
    \label{fig:pipline}
\end{figure}

\section{Experiments}
We selected commonly used training datasets as the experimental foundation. The initial candidate set included datasets such as GSM8K and others, from which we ultimately filtered out 5 core datasets for subsequent experiments. In the dataset selection process, we particularly emphasized the coverage of data, ensuring that the selected datasets cover mathematical computation, common-sense reasoning, and task types that require complete logical reasoning processes. This design enables the model to more comprehensively expose performance characteristics under different task scenarios during training, thereby more accurately validating the actual effectiveness of our proposed method across various training tasks.

The specific information about each dataset is shown in Table~\ref{tab:datasets}.

\begin{table}[htbp]
\centering
\caption{Dataset Statistics}
\label{tab:datasets}
\begin{tabular}{@{}lcc@{}}
\hline
\textbf{Dataset Name} & \textbf{Domain} & \textbf{Size} \\
\hline
AQuA-RAT (aqua\_cot) & Math Reasoning & 97,467 \\
GSM8K & Math Problem Solving & 7,473 \\
OpenBookQA & Open-domain QA & 4,957 \\
SimpleQA & Simple QA & 4,326 \\
SVAMP & Math Word Problems & 700 \\
\hline
\end{tabular}
\end{table}

In the experiments, we developed automated training scripts based on the Llama Factory framework, which were used to perform cyclic continuous fine-tuning on the aforementioned 5 datasets. The training process consists of two stages: the first stage adopts conventional LoRA training methods; the second stage introduces our improved low-perplexity Mask strategy, specifically including two implementation approaches: top perplexity Mask and bottom perplexity Mask.

Additionally, the key parameter settings during the training process are as follows: the number of training epochs is set to 3, the learning rate is $1.0 \times 10^{-4}$, the batch size is 16 with gradient accumulation steps of 4 (resulting in an effective batch size of 64). For LoRA configuration, we set the rank to 16, alpha to 16, dropout to 0.05, and target modules to all linear layers.

Finally, for the CFT models trained in this experiment, we conducted three core evaluations: (1) the model's basic performance on training tasks; (2) the model's performance on the first-stage training datasets after second-stage training, represented by ROUGE\_L; and (3) visualization of the model's performance on first-stage training datasets through heatmaps. It should be particularly noted that the data metrics in the heatmaps are primarily used to measure the performance retention of the model on first-stage training datasets after second-stage CFT training. This metric serves as the core basis for evaluating whether our proposed method can effectively alleviate the model's ``catastrophic forgetting'' problem.

\subsection{Training Loss Analysis}

From the loss curves, taking two models as examples (GSM8K-to-SimpleQA and SVAMP-to-SimpleQA), we can observe that the two low-perplexity training methods exhibit faster convergence compared to standard LoRA transfer. The low-perplexity training approaches lead to lower training losses, and the mask bottom 10 method achieves lower loss than the mask highest method. This indicates that selectively masking high-perplexity tokens (bottom 10\%) is more effective in reducing training loss and improving convergence speed compared to masking the highest perplexity tokens.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LaTeX/stage2_loss_comparison_gsm8k_to_simpleQA.png}
    \caption{Stage 2 Training Loss Comparison: GSM8K to SimpleQA}
    \label{fig:loss_gsm8k_simpleqa}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LaTeX/stage2_loss_comparison_svamp_to_simpleQA.png}
    \caption{Stage 2 Training Loss Comparison: SVAMP to SimpleQA}
    \label{fig:loss_svamp_simpleqa}
\end{figure}

\section{Conclusion}

This work focuses on performance optimization and catastrophic forgetting mitigation in models under continuous fine-tuning (CFT) scenarios using low perplexity, achieving research objectives through two key technical innovations. The main conclusions are as follows:

First, this study successfully transplanted the perplexity and confidence concepts from DeepConfidence into the model fine-tuning process, effectively improving the stability and task adaptability of the CFT training process, providing a reliable technical foundation for the implementation of subsequent improvement strategies. Second, addressing the widespread catastrophic forgetting problem in CFT training, this study introduced and validated the effectiveness of two perplexity Mask strategies---bottom-10\% perplexity Mask and highest perplexity Mask. Both strategies can specifically suppress the model's forgetting of previously trained knowledge.

On this basis, the automated CFT training script constructed in this study achieves full automation from data processing to training output, significantly improving experimental efficiency and reproducibility. Experimental results further confirm that the aforementioned two perplexity Mask strategies can not only effectively alleviate catastrophic forgetting in CFT but also simultaneously improve the model's training performance, significantly outperforming conventional CFT training methods in both task adaptability and knowledge retention metrics.

\subsection{Limitations}

The experimental design and result validation of this study are constrained by training resource conditions, with two main limitations:

First, the model size selection is limited. Due to computational resource constraints, this study only conducted LoRA fine-tuning experiments on small-scale models with 1B~1.5B parameters. Small-parameter models inherently have limitations in their representational capacity and learning capability, making it difficult to fully realize the potential of the proposed method.

Second, the training data scope is limited. Constrained by the input processing capacity of small-scale models, this study could not utilize longer-sequence Chain-of-Thought (CoT) data for training. This not only limits the diversity and complexity of training data but may also result in insufficient validation of the model's performance on long logical chain reasoning tasks, thereby affecting the upper bound of the final performance of the proposed method.

\subsection{Future Work}

Addressing the aforementioned limitations and building upon the technical framework of this study, future research can explore the following directions:

First, expanding model size and training scale. Future work could validate the effectiveness of the proposed CFT method on larger parameter-scale models (e.g., 7B, 13B, etc.). Larger model capacity is expected to better accommodate knowledge retention and performance improvement requirements during continuous learning, further unleashing the value of low-perplexity Mask strategies.

Second, introducing long-sequence CoT data training. We can attempt to apply the low-perplexity Mask method on longer-sequence CoT data. By extending the length of input strings, we can provide the model with more comprehensive long logical chain reasoning learning samples, helping the model improve its reasoning capabilities for complex tasks.

Third, expanding method application scenarios. The low-perplexity Mask strategy and automated CFT pipeline proposed in this study can be extended to the Reinforcement Learning (RL) stage, exploring their adaptability in a joint framework of ``continuous fine-tuning---reinforcement learning,'' further enriching the application boundaries and practical value of the method.

\bibliography{references}

\end{document}
