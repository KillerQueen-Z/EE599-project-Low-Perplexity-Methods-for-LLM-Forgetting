% Standard LaTeX table format (no additional packages required)
% Complete document section with dataset description and table
%
% Note on paragraph indentation:
% LaTeX automatically indents the first line of each paragraph.
% Empty lines (blank lines) between paragraphs indicate new paragraphs.
% IMPORTANT: By default, LaTeX does NOT indent the first paragraph after
% \section{} or \subsection{}. To make the first paragraph also indented:
%   Option 1: Add \usepackage{indentfirst} in your document preamble
%   Option 2: Use \indent before the first paragraph (as done in this file)
% To control indentation, you can add in your document preamble:
%   \setlength{\parindent}{1em}     % Set indentation size (default ~1.5em)
%   \noindent                        % Use before a paragraph to remove indentation

We selected commonly used training datasets as the experimental foundation. The initial candidate set included datasets such as GSM8K and others, from which we ultimately filtered out 5 core datasets for subsequent experiments. In the dataset selection process, we particularly emphasized the coverage of data, ensuring that the selected datasets cover mathematical computation, common-sense reasoning, and task types that require complete logical reasoning processes. This design enables the model to more comprehensively expose performance characteristics under different task scenarios during training, thereby more accurately validating the actual effectiveness of our proposed method across various training tasks.

The specific information about each dataset is shown in Table~\ref{tab:datasets}.

\begin{table}[htbp]
\centering
\caption{Dataset Statistics}
\label{tab:datasets}
\begin{tabular}{@{}lcc@{}}
\hline
\textbf{Dataset Name} & \textbf{Domain} & \textbf{Size} \\
\hline
AQuA-RAT (aqua\_cot) & Math Reasoning & 97,467 \\
GSM8K & Math Problem Solving & 7,473 \\
OpenBookQA & Open-domain QA & 4,957 \\
SimpleQA & Simple QA & 4,326 \\
SVAMP & Math Word Problems & 700 \\
\hline
\end{tabular}
\end{table}

In the experiments, we developed automated training scripts based on the Llama Factory framework, which were used to perform cyclic continuous fine-tuning on the aforementioned 5 datasets. The training process consists of two stages: the first stage adopts conventional LoRA training methods; the second stage introduces our improved low-perplexity Mask strategy, specifically including two implementation approaches: top perplexity Mask and bottom perplexity Mask.

Additionally, the key parameter settings during the training process are as follows: the number of training epochs is set to 3.

Finally, for the CFT models trained in this experiment, we conducted three core evaluations: (1) the model's basic performance on training tasks; (2) the model's performance on the first-stage training datasets after second-stage training, represented by ROUGE\_L; and (3) visualization of the model's performance on first-stage training datasets through heatmaps. It should be particularly noted that the data metrics in the heatmaps are primarily used to measure the performance retention of the model on first-stage training datasets after second-stage CFT training. This metric serves as the core basis for evaluating whether our proposed method can effectively alleviate the model's ``catastrophic forgetting'' problem.

\section{Conclusion}

\indent This work focuses on performance optimization and catastrophic forgetting mitigation in models under continuous fine-tuning (CFT) scenarios using low perplexity, achieving research objectives through two key technical innovations. The main conclusions are as follows:

First, this study successfully transplanted the perplexity and confidence concepts from DeepConfidence into the model fine-tuning process, effectively improving the stability and task adaptability of the CFT training process, providing a reliable technical foundation for the implementation of subsequent improvement strategies. Second, addressing the widespread catastrophic forgetting problem in CFT training, this study introduced and validated the effectiveness of two perplexity Mask strategies---bottom-10\% perplexity Mask and highest perplexity Mask. Both strategies can specifically suppress the model's forgetting of previously trained knowledge.

On this basis, the automated CFT training script constructed in this study achieves full automation from data processing to training output, significantly improving experimental efficiency and reproducibility. Experimental results further confirm that the aforementioned two perplexity Mask strategies can not only effectively alleviate catastrophic forgetting in CFT but also simultaneously improve the model's training performance, significantly outperforming conventional CFT training methods in both task adaptability and knowledge retention metrics.

\subsection{Limitations}

\indent The experimental design and result validation of this study are constrained by training resource conditions, with two main limitations:

First, the model size selection is limited. Due to computational resource constraints, this study only conducted LoRA fine-tuning experiments on small-scale models with 1B~1.5B parameters. Small-parameter models inherently have limitations in their representational capacity and learning capability, making it difficult to fully realize the potential of the proposed method.

Second, the training data scope is limited. Constrained by the input processing capacity of small-scale models, this study could not utilize longer-sequence Chain-of-Thought (CoT) data for training. This not only limits the diversity and complexity of training data but may also result in insufficient validation of the model's performance on long logical chain reasoning tasks, thereby affecting the upper bound of the final performance of the proposed method.

\subsection{Future Work}

\indent Addressing the aforementioned limitations and building upon the technical framework of this study, future research can explore the following directions:

First, expanding model size and training scale. Future work could validate the effectiveness of the proposed CFT method on larger parameter-scale models (e.g., 7B, 13B, etc.). Larger model capacity is expected to better accommodate knowledge retention and performance improvement requirements during continuous learning, further unleashing the value of low-perplexity Mask strategies.

Second, introducing long-sequence CoT data training. We can attempt to apply the low-perplexity Mask method on longer-sequence CoT data. By extending the length of input strings, we can provide the model with more comprehensive long logical chain reasoning learning samples, helping the model improve its reasoning capabilities for complex tasks.

Third, expanding method application scenarios. The low-perplexity Mask strategy and automated CFT pipeline proposed in this study can be extended to the Reinforcement Learning (RL) stage, exploring their adaptability in a joint framework of ``continuous fine-tuning---reinforcement learning,'' further enriching the application boundaries and practical value of the method.
